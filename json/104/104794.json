{"code": 0, "data": [{"cid": 503555, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Alice", "pid": 104794, "reply_to": -1, "tag": null, "text": "为啥不能在一个洞的回复里发呀呜呜呜", "timestamp": 1604067258, "type": "text", "url": ""}, {"cid": 503559, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 104794, "reply_to": -1, "tag": null, "text": "Re Alice: 抱歉我没想到评论区也可以发markdown这件事，一定是我的脑子太累了(doge", "timestamp": 1604067341, "type": "text", "url": ""}, {"cid": 506779, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Bob", "pid": 104794, "reply_to": -1, "tag": null, "text": "#103422 的洞主前来报道。果然洞里人才多啊", "timestamp": 1604155329, "type": "text", "url": ""}, {"cid": 506822, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Bob", "pid": 104794, "reply_to": -1, "tag": null, "text": "洞主是上传了三个版本吗？", "timestamp": 1604155788, "type": "text", "url": ""}, {"cid": 506827, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Bob", "pid": 104794, "reply_to": -1, "tag": null, "text": "啊看到注释了。", "timestamp": 1604155861, "type": "text", "url": ""}, {"cid": 622092, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Carol", "pid": 104794, "reply_to": -1, "tag": null, "text": "carol写了个更方便的版本，该如何发上来呢", "timestamp": 1606465464, "type": "text", "url": ""}, {"cid": 622156, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Carol", "pid": 104794, "reply_to": -1, "tag": null, "text": "这个版本加入了多线程，直接下载某个搜索关键词的全部套图，每次只需要修改搜索关键词并运行即可\n```python\nimport os\nimport requests\nimport re\nimport threading\nfrom urllib.request import quote\n\ntitle = 'xiurenji'\n\n# 引号内输入搜索关键词，然后运行\nkeyword = ''\n# 这里输入存图路径\nsave_dir = f'./{title}/'\n\nurl = f'https://www.xiurenji.com/plus/search/index.asp?keyword={keyword}&searchtype=title'\nurl = quote(url, safe=\";/?:@&=+$,\", encoding=\"gbk\")\n\ndef get_links(html):\n    '''\n        从搜索页面的html代码里获取所有图集的超链接页面\n        [[url, image_title], ...]\n    '''\n    url_root = r'https://www.xiurenji.com'\n\n    pattern = r'<div class=\"title1\"><a href=\"(?P<href>.*)\">(?P<title>.*)</a></div>'\n    re_result = re.findall(pattern, html)\n\n    result = []\n    for i in range(len(re_result)):\n        result.append([\n            url_root + re_result[i][0],\n            re_result[i][1].replace('<font color=red>', '').replace('</font>', '')\n        ])\n    return result\n\ndef get_search_page_links(html):\n    '''\n        从搜索页面的html代码里获取所有分页的页面链接\n        [[url, page_num], ...]\n    '''\n    url_root = r'https://www.xiurenji.com/plus/search/index.asp'\n\n    pattern = r'<!--分页开始-->.*<!--分页结束-->'\n    re_result = re.findall(pattern, html, re.S)\n\n    html = re_result[0]\n    pattern = r'<a href=\"(?P<href>.*?)\".*?>(?P<title>.*?)</a>'\n    re_result = re.findall(pattern, html)\n\n    result = []\n    for i in range(len(re_result)):\n        result.append([\n            url_root + re_result[i][0],\n            re_result[i][1]\n        ])\n    return result\n\ndef get_imageset_page_links(html):\n    '''\n        从图集页面的html代码里获取所有分页的页面链接\n        [[url, page_num], ...]\n    '''\n    url_root = r'https://www.xiurenji.com'\n\n    pattern = r'<div class=\"page\">(.*?)</div>'\n    re_result = re.findall(pattern, html, re.S)\n\n    html = re_result[0]\n    pattern = r'<a href=\"(?P<href>.*?)\".*?>(?P<title>.*?)</a>'\n    re_result = re.findall(pattern, html)\n\n    result = []\n    for i in range(len(re_result)):\n        result.append([\n            url_root + re_result[i][0],\n            re_result[i][1]\n        ])\n    return result[:-1]  # 去掉\"下一页\"\n\ndef download_img(url, path):\n    '''\n        下载url的文件到path路径\n    '''\n    r = requests.get(url, stream=True)\n    if r.status_code == 200:\n        open(path, 'wb').write(r.content)  # 将内容写入图片\n\ndef parse_page(html, dir, img_count):\n    '''\n        从图片的html中获取信息，返回下一张图的编号\n    '''\n    url_root = r'https://www.xiurenji.com'\n\n    pattern = r'<img onload=.*?src=\"(.*?)\" /><br />'\n    re_result = re.findall(pattern, html, re.S)\n\n    result = []\n    for i in range(len(re_result)):\n        result.append(url_root + re_result[i])\n\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n    for url in result:\n        download_img(url, os.path.join(dir, '%03d.jpg' % img_count))\n        img_count += 1\n\n    return img_count\n\ndef process_imageset(page_url, page_title):\n    print(f'正在下载 {page_title} 的图片...')\n    html = requests.get(page_url).text\n    imageset_page_links = get_imageset_page_links(html)\n    # 获取每个套图每个页面的图片\n    img_count = 0\n    for img_url, img_page_num in imageset_page_links:\n        img_html = requests.get(img_url).text\n        img_count = parse_page(img_html, os.path.join(save_dir, page_title), img_count)\n\n\ndef main():\n    global save_dir\n    save_dir = os.path.join(save_dir, keyword)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    home_page = requests.get(url).text\n    page_links = get_search_page_links(home_page)\n\n    # 获取所有套图的url\n    print('获取套图列表...')\n    imageset_links = []\n    for page_url, page_num in page_links:\n        html = requests.get(page_url).text\n        imageset_links.extend(get_links(html))\n    print(f'共 {len(imageset_links)} 套图.')\n\n    # 获取每个套图的页面url\n    threads = []\n    for page_url, page_title in imageset_links:\n        th = threading.Thread(target=process_imageset, args=(page_url, page_title))\n        threads.append(th)\n\n    for th in threads:\n        th.start()\n    for th in threads:\n        th.join()\n\n    print('下载完成.')\n\nif __name__ == '__main__':\n    main()\n```", "timestamp": 1606466110, "type": "text", "url": ""}], "post": {"deleted": false, "image_metadata": {}, "likenum": 8, "pid": 104794, "reply": 7, "tag": "NSFW", "text": "#NSFW#\n#兔女郎与猫娘的日常#\n我是兔女郎与猫娘，#104574里说的代码我写好了……丑话先说在前面，dz没有接受过系统训练，学的专业和编程不沾边，学python依靠的全是b站上的教程和一颗lsp的心。这是我第一次写代码给别人用，肯定会各种bug满天飞，我尽量及时回复质疑，保证虚心接受批评。\n另外我发现树洞贴代码，如果一行代码太长的话排版会乱套，但是复制到python里缩进还是正常的，所以不用担心。由于单个树洞的字数限制，我不得不分成多个树洞发，大家自行在python里拼起来吧……\n```python\n#########################################################################################\n\n\n####如果您没有安装lxml模块，请看这里：\n\n#如果没有安装lxml模块，很遗憾您只能一个套图一个套图的下载，下面的程序功能为：\n\n#输入套图的第一页网址 和 图片保存的路径，程序会帮您把这份套图全部下载下来，一次只能下载一套\n\n#为了避开lxml包，我们使用re模块的正则表达式来爬取，如果这个模块您也不能成功调用，那么很遗憾我也无能为力了\n\nimport re#(同时别忘了运行上面的requests和os模块)\n\n####下面是爬虫主体，使用前先运行\nclass xiuren(object):\n       def __init__(self,url,path):\n              self.url=url\n              self.path=path\n              self.number=re.match('.*(\\d{4})',self.url).group(1)\n              self.publish=re.match('https://www.xiurenji.com/(.*)/',self.url).group(1)\n              self.headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n              pass    \n       def get_page(self,url):\n              response=requests.get(url,headers=self.headers)\n              match='<a href=\"/'+self.publish+'/'+self.number+'_\\d{1,3}'\n              res=re.findall(match.encode('utf-8'),response.content)\n              #找到其中的最大页码\n              a=0\n              for i in res:\n                     match=re.match('.*'+self.number+'_(\\d{1,3})',i.decode('utf-8'))\n                     b=int(match.group(1))\n                     if b>a:\n                            a=b\n                            pass\n                     pass\n              return(a)\n              pass\n       def find_img(self,url):#给定网址，返回这一网址三张图片的地址\n              response=requests.get(url,headers=self.headers)\n              res=re.findall('/uploadfile.*jpg'.encode('utf-8'),response.content)#找到了图片的地址\n              return res\n       def save_img(self,page,url):\n              name=1#用来命名每个图片\n              if os.path.exists(self.path+'\\\\'+self.number)==False:\n                     os.mkdir(self.path+'\\\\'+self.number)\n              for i in range(page+1):\n                     if i==0:\n                            res=self.find_img(url)\n                            pass\n                     if i>0:\n                            res=self.find_img(self.url.replace('.html','')+'_'+str(i)+'.html')\n                            pass\n                     for j in res:\n                            web='https://www.xiurenji.com'+j.decode('utf-8')\n                            response=requests.get(web)\n                            with open(self.path+'\\\\'+self.number+'\\\\'+str(name)+'.jpg','wb') as f:#图片也是二进制所以用wb写入\n                                   f.write(response.content)\n                                   pass\n                            name=name+1\n                            pass\n                     pass\n              print('下载完成，感谢使用~')\n              return\n       pass\n\n##运行完成上面的爬虫主体后，就可以进行下载了\n\n##打开浏览器任意一组套图的第一页，复制网址输入\n\n##同时需要输入你要保存的文件夹，运行前请确定该文件夹存在！下面是一个输入范例，运行下面两行程序即可保存\n       \n #保存 https://www.xiurenji.com/MyGirl/6142.html（不知道这是什么可以浏览器打开看看，就是一个套图的第一页） 中的所有图片到对应文件夹中，使用前请确保该文件夹存在，地址是两个斜杠！\na=xiuren('https://www.xiurenji.com/MyGirl/6142.html','E:\\\\文件夹A')\na.save_img(page=a.get_page(a.url),url=a.url)", "timestamp": 1604067185, "type": "text", "updated_at": 1606782636, "url": "", "vote": {}}}