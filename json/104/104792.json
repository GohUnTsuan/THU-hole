{"code": 0, "data": [], "post": {"deleted": false, "image_metadata": {}, "likenum": 6, "pid": 104792, "reply": 0, "tag": "NSFW", "text": "#NSFW#\n#兔女郎与猫娘的日常#\n我是兔女郎与猫娘，#104574里说的代码我写好了……丑话先说在前面，dz没有接受过系统训练，学的专业和编程不沾边，学python依靠的全是b站上的教程和一颗lsp的心。这是我第一次写代码给别人用，肯定会各种bug满天飞，我尽量及时回复质疑，保证虚心接受批评。\n另外我发现树洞贴代码，如果一行代码太长的话排版会乱套，但是复制到python里缩进还是正常的，所以不用担心。由于单个树洞的字数限制，我不得不分成多个树洞发，大家自行在python里拼起来吧……\n```python\n##################################################################################\n##如果您有lxml模块，但是没有selenium模块，问题不大，只是需要您自行搜索而已\n\n#例如您想要搜索糯美子，那么请自行打开浏览器进入https://www.xiurenji.com/plus/search/index.asp（必须在这里搜索！在首页右上角直接搜索是不行的\n\n#在左上角（注意是左上角！不是右上角！）搜索梦心月，会弹出搜索结果，将第一页搜索结果的网址传入即可\n\n#例如梦心月的第一页网址为：https://www.xiurenji.com/plus/search/index.asp?keyword=%C3%CE%D0%C4%D4%C2&searchtype=title(不知道我在说什么可以把这个网址打开看看，就是搜索结果的第一页)\n\n#其余需要输入的参数（name,model_path,set_start,set_num,notice）与上面一致\n\n#下面是爬虫主体：\nclass modelspider_v2(object):\n       def __init__(self,name,model_path,html_search,set_start=1,set_num=1,notice='yes'):\n              self.name=name#模特艺名\n              self.set_start=set_start#从第几组套图开始下载\n              self.set_num=set_num#下载set_num组套图，默认下载5套，如果全部下载则输入'max'\n              self.notice=notice#每下载完成一套图是否询问要不要继续\n              self.model_path=model_path#保存文件夹的目录\n              self.html_search=html_search\n              self.headers={#requests headers\n                            'Accept':\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n                            'Accept-Encoding':'gzip, deflate, br',\n                            'Accept-Language':\"zh-CN,zh;q=0.9\",\n                            'Cache-Control':\"max-age=0\",\n                            'Connection':\"keep-alive\",\n                            'Cookie':\"UM_distinctid=173d30553d544e-05e8d2425a679e-3323765-e1000-173d30553d64cf; __51cke__=; ASPSESSIONIDAWAADQTB=NIFBIBABGHEHMNBIFLHHLOJI; CNZZDATA1278618868=278254405-1596971648-https%253A%252F%252Fwww.baidu.com%252F%7C1603939602; __tins__20641871=%7B%22sid%22%3A%201603939974060%2C%20%22vd%22%3A%2020%2C%20%22expires%22%3A%201603942451275%7D; __51laig__=71\",\n                            'Host':\"www.xiurenji.com\",\n                            'Sec-Fetch-Dest':\"document\",\n                            'Sec-Fetch-Mode':\"navigate\",\n                            'Sec-Fetch-Site':\"none\",\n                            'Sec-Fetch-User':\"?1\",\n                            'Upgrade-Insecure-Requests':\"1\",\n                            'User-Agent':\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36\",\n                            }\n              pass\n       def get_set_page_list(self):\n              html=requests.get(self.html_search,headers=self.headers).content\n              html=etree.HTML(html)\n              set_page_list=[]\n              #判断一下有没有足够多的set_num，如果没有则默认全下完\n              temp_num_max=len(html.xpath(\".//div[@class='page']//a/@href\"))\n              if temp_num_max==0:\n                     #没搜到\n                     self.set_page_list=set_page_list\n                     return\n              if (math.ceil((self.set_start+self.set_num-1)/10))>(temp_num_max):\n                     print('主人搜索的小姐姐没有'+str(self.set_num)+'组那么多套图哦，我默认下载全部啦~')\n                     self.set_num=='max'\n              if self.set_num=='max':              \n                     for i in html.xpath(\".//div[@class='page']//a/@href\")[math.ceil(self.set_start/10)-1:]:\n                            set_page_list.append('https://www.xiurenji.com/plus/search/index.asp'+i)\n                            pass\n                     pass\n              else:\n                     for i in html.xpath(\".//div[@class='page']//a/@href\")[math.ceil(self.set_start/10)-1:math.ceil((self.set_start+self.set_num-1)/10)]:\n                            set_page_list.append('https://www.xiurenji.com/plus/search/index.asp'+i)\n                            pass\n                     pass\n              self.set_page_list=set_page_list\n              return\n       def get_set_list(self,set_page_list):\n              #输入set_page_list，导出每一组套图的地址\n              set_list=[]\n              #顺便存一下每组套图的名字，保存文件名要用\n              set_name_list=[]\n              for set_page in set_page_list:\n                     req=requests.get(set_page,headers=self.headers)\n                     response=req.content\n                     response=etree.HTML(response)\n                     for i in response.xpath(\".//div[@class='title1']//a/@href\"):\n                            set_list.append('https://www.xiurenji.com'+i)\n                            pass\n                     temp_name=response.xpath(\".//div[@class='title1']//a/text()\")\n                     for j in range(int(len(temp_name)/2)):\n                            text=temp_name[j*2]+self.name+temp_name[j*2+1]\n                            set_name_list.append(text)\n                            pass\n                     pass\n              temp_num_max=len(set_list)\n              if (int(self.set_start%10)-1+self.set_num)>(temp_num_max-1):\n                     print('主人搜索的小姐姐没有'+str(self.set_num)+'组那么多套图哦，我默认下载全部啦~')\n                     self.set_num=='max'\n              if self.set_num=='max':\n                     set_list=set_list[int(self.set_start%10)-1:]\n                     set_name_list=set_name_list[int(self.set_start%10)-1:]\n                     pass\n              if self.set_num!='max':\n                     set_list=set_list[int(self.set_start%10)-1:int(self.set_start%10)-1+self.set_num]\n                     set_name_list=set_name_list[int(self.set_start%10)-1:int(self.set_start%10)-1+self.set_num]\n                     pass\n              self.set_list=set_list\n              self.set_name_list=set_name_list\n              return\n       def get_img_page_list(self,set_url):\n              #输入套图地址，下载该组套图的每一页地址\n              img_page_list=[]\n              req=requests.get(set_url,headers=self.headers)\n              response=req.content\n              response=etree.HTML(response)\n              temp=response.xpath(\".//div[@class='page']//a/@href\")\n              for i in temp[0:int((len(temp)/2))-1]:\n                     img_page_list.append('https://www.xiurenji.com'+i)\n                     pass\n              pass\n              self.img_page_list=img_page_list\n              return\n       def get_img_download_list(self,img_page_list):\n              #输入套图每一页地址表单，输出每一页三张图下载地址表单\n              img_download_list=[]\n              for img_page in img_page_list:\n                     req=requests.get(img_page,headers=self.headers)\n                     response=req.content\n                     response=etree.HTML(response)\n                     for i in response.xpath(\".//div[@class='img']//img/@src\"):\n                            img_download_list.append('https://www.xiurenji.com'+i)\n                            pass\n                     pass\n              self.img_download_list=img_download_list\n              return \n       def save_img(self,img_name,img_path,img_download):\n              #输入单个图片地址，在要求的路径中保存为指定名称\n              req=requests.get(img_download,headers=self.headers)\n              with open(img_path+'\\\\'+img_name+'.jpg', 'wb') as f:\n                     f.write(req.content)\n                     pass\n              return\n       def main_func(self):\n              self.get_set_page_list()\n              self.get_set_list(self.set_page_list)\n              #创建模特的文件夹\n              if os.path.exists(self.model_path+'\\\\'+self.name)==False:\n                     model_path=self.model_path+'\\\\'+self.name\n                     os.mkdir(model_path)              \n              for i in range(self.set_num):\n                     #创建套图名称文件夹\n                     if os.path.exists(self.model_path+'\\\\'+self.name+'\\\\'+self.set_name_list[i])==False:\n                            set_path=self.model_path+'\\\\'+self.name+'\\\\'+self.set_name_list[i]\n                            os.mkdir(set_path)\n                     self.get_img_page_list(self.set_list[i])\n                     self.get_img_download_list(self.img_page_list)\n                     img_name=1\n                     for img_download in self.img_download_list:\n                            self.save_img(str(img_name),set_path,img_download)\n                            img_name=img_name+1\n                            pass\n                     if self.notice=='no':\n                            print('第'+str(i+1)+'组图下载完成啦，继续下载下一套咯，主人可以先去看看'+'\\n')\n                     if i==(self.set_num-1):\n                            print('全部下载完成啦，感谢使用~')\n                            return\n                     if self.notice=='yes':\n                            temp_notice=input('第'+str(i+1)+'组图下载完成啦，请问是否继续下载下一套？输入yes继续，输入no终止哦~'+'\\n')\n                            while (temp_notice!='yes')&(temp_notice!='no'):\n                                   temp_notice=input('啊咧？要输入yes或no的鸭'+'\\n')\n                            if temp_notice=='no':\n                                   print('感谢使用，呜喵~')\n                                   return\n                            else:\n                                   self.notice=input('下载完成下一套图后是否还要继续询问主人是否继续呢？仍要询问输入yes，不再询问输入no哦~'+'\\n')\n                                   while (self.notice!='yes')&(self.notice!='no'):\n                                          self.notice=input('啊咧？要输入yes或no的鸭'+'\\n')\n                                   print('收到，那我继续')\n                     pass\n              return\n       pass\n\n\n\n####运行完上面的爬虫主体后，就可以用下面的程序进行下载了，下面是一个范例，点击运行下面两行会下载梦心月的第8套图和第9套图（从第8套开始下载两套），保存在指定文件夹（运行前请确定该文件夹存在！地址是两个斜杠！\nmodel2=modelspider_v2(name='梦心月',model_path='E:\\\\文件夹A',html_search='https://www.xiurenji.com/plus/search/index.asp?keyword=%C3%CE%D0%C4%D4%C2&searchtype=title',set_start=8,set_num=2,notice='yes')       \nmodel2.main_func()", "timestamp": 1604067101, "type": "text", "updated_at": 1606782638, "url": "", "vote": {}}}