{"code": 0, "data": [{"cid": 1242741, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Alice", "pid": 244945, "reply_to": -1, "tag": null, "text": "同不会，蹲一个", "timestamp": 1615516584, "type": "text", "url": ""}, {"cid": 1242747, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 244945, "reply_to": -1, "tag": null, "text": "个人觉得tanh更好，但是为何不常用呢", "timestamp": 1615516647, "type": "text", "url": ""}, {"cid": 1242753, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Alice", "pid": 244945, "reply_to": -1, "tag": null, "text": "可能是因为tanh相较sigmoid横向压缩了，所以梯度下降的速度更慢（？）", "timestamp": 1615516740, "type": "text", "url": ""}, {"cid": 1242795, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Bob", "pid": 244945, "reply_to": -1, "tag": null, "text": "0比-1更有意义吧", "timestamp": 1615517350, "type": "text", "url": ""}, {"cid": 1243235, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Carol", "pid": 244945, "reply_to": -1, "tag": null, "text": "花点时间说一些问题吧。\n1. 先明确所谓sigmoid指什么。现在ML社区似乎绝大多数人喜欢把sigmoid等同于logistic sigmoid，实际上你去wiki上看sigmoid本身指的是所有“S”型函数，这也包括tanh。我默认dz说的是logistic function和tanh的对比了。\n2. 近年很少看到有再去研究所谓激活函数的工作了，做theory的基本都用ReLU，deep相关的实践中默认应该也都是ReLU。为什么呢？1. ReLU最好算梯度（不论是理论还是实验，虽然在0处的subgradient的处理饱受数学家诟病） 2. 因为BN/LN的引入，不用再担心以前很多人在意的所谓gradient explosion/vanishing问题了（参考比如Li and Arora 2020），选什么激活函数不是很有所谓（参考BN原文用sigmoid也训出很好的结果）。\n3. 所以dz其实问的是个前BN时代的问题，因为logistic sigmoid简单地rescale一下就变成tanh了。我印象前ReLU时代也是tanh似乎更受欢迎啊，因为logistic sigmoid是non-zero mean的，积累下去就gradient vanishing了，BN之后才没这个问题。\n4. No Free Lunch Theorem. 不存在任何A方法【绝对】比B方法好的情形，都要看你要处理的数据集的性质，还有处理起来是否方便。所以现在非deep的模型一样用sigmoid挺多。\n最后，以上说的其实很可能都是胡扯，至今关于BN或者激活函数并没有太好的有完整框架的理论工作（难做），只是从实验上我们知道BN/LN+ReLU的确好，所以很少有人再去在意sigmoid和tanh在MLP到底谁更“好”这种问题了……", "timestamp": 1615522808, "type": "text", "url": ""}, {"cid": 1243454, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 244945, "reply_to": -1, "tag": null, "text": "Re Carol: 跪", "timestamp": 1615525460, "type": "text", "url": ""}, {"cid": 1243976, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Dave", "pid": 244945, "reply_to": -1, "tag": null, "text": "Re Carol: 膜carol！是yao班大佬吗？能分享一下近期deep learning theory的研究方向吗？", "timestamp": 1615531788, "type": "text", "url": ""}, {"cid": 1246431, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Carol", "pid": 244945, "reply_to": -1, "tag": null, "text": "Re Dave: 前ntk时代在做哪些工作现在大概还是做那些（\n此外就是去年水self-supervised的比较多 今年或没水头了", "timestamp": 1615555381, "type": "text", "url": ""}, {"cid": 1246562, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Dave", "pid": 244945, "reply_to": -1, "tag": null, "text": "Re Carol: 厉害！你是大几或者在哪读博呢？\n你觉得现在optimization还有做头吗？（在over parameterized下全局最优的工作之后）", "timestamp": 1615556605, "type": "text", "url": ""}], "post": {"deleted": false, "image_metadata": {}, "likenum": 7, "pid": 244945, "reply": 9, "tag": null, "text": "多层感知器的激活函数为什么用sigmoid比tanh更好？", "timestamp": 1615516322, "type": "text", "updated_at": 1615556603, "url": "", "vote": {}}}