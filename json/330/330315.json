{"code": 0, "data": [{"cid": 1681824, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Alice", "pid": 330315, "reply_to": -1, "tag": null, "text": "不是很正常吗，batch_size越大，越稳定梯度下降", "timestamp": 1621346623, "type": "text", "url": ""}, {"cid": 1682056, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Bob", "pid": 330315, "reply_to": -1, "tag": null, "text": "你测试的时候用net.eval()了吗？\n这种情况很有可能是用了batchnorm忘记eval了吧", "timestamp": 1621348305, "type": "text", "url": ""}, {"cid": 1696948, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 330315, "reply_to": 1682056, "tag": null, "text": "Re Bob: 对了，就是这个情况，现在解决了，感谢Bob", "timestamp": 1621573113, "type": "text", "url": ""}], "post": {"deleted": false, "image_metadata": {}, "likenum": 4, "pid": 330315, "reply": 3, "tag": null, "text": "神经网络问题求教：\n当前训练时用的是16的batch_size，得到的模型，测试的时候用的是1的batch_size，发现结果烂的要死，而batch_size调大结果会慢慢好起来，调到32时甚至比16要好，这是为什么呢？\n（背景是Finetune一个ResNet）", "timestamp": 1621345064, "type": "text", "updated_at": 1621573112, "url": "", "vote": {}}}