{"code": 0, "data": [{"cid": 2189432, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": -1, "tag": null, "text": "还有优化器是如何做到优化model.parameters()的呢，刚刚发现parameters()返回的是一个generator类型，并不是一个引用类型，修改的是拷贝的值，那model里面的参数为什么会改变呢", "timestamp": 1627875576, "type": "text", "url": ""}, {"cid": 2189449, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Alice", "pid": 424210, "reply_to": -1, "tag": null, "text": "反向传播更新权重", "timestamp": 1627875684, "type": "text", "url": ""}, {"cid": 2189473, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Bob", "pid": 424210, "reply_to": -1, "tag": null, "text": "这个是pytorch内部实现的，不用管，按照语法写就行", "timestamp": 1627875833, "type": "text", "url": ""}, {"cid": 2189485, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2189449, "tag": null, "text": "Re Alice: 如果是更新model的权重，应该是调用model的成员函数进行更新呀。loss虽然是用model(X)和y计算出来的，但是直接调用loss的成员函数就能更新model的餐食这个操作感觉很迷惑。", "timestamp": 1627875891, "type": "text", "url": ""}, {"cid": 2189487, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Carol", "pid": 424210, "reply_to": -1, "tag": null, "text": "实体化model的时候，会构建一个计算图，.backward是一个内置的函数，将梯度回传。优化器就是控制梯度用的，也可以当做包装好的一个东西。", "timestamp": 1627875898, "type": "text", "url": ""}, {"cid": 2189489, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Dave", "pid": 424210, "reply_to": -1, "tag": null, "text": "可以用ctrl+鼠标左键查看源码", "timestamp": 1627875905, "type": "text", "url": ""}, {"cid": 2189494, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2189473, "tag": null, "text": "Re Bob: 可是好想搞明白呀。看pytorch里面的源码也没看懂", "timestamp": 1627875923, "type": "text", "url": ""}, {"cid": 2189498, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Carol", "pid": 424210, "reply_to": -1, "tag": null, "text": "dz文科生居然想这么多，佩服佩服！厉害的！", "timestamp": 1627875940, "type": "text", "url": ""}, {"cid": 2189504, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Alice", "pid": 424210, "reply_to": 2189485, "tag": null, "text": "Re 洞主: backward函数里就对权重进行了更新", "timestamp": 1627875976, "type": "text", "url": ""}, {"cid": 2189506, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Dave", "pid": 424210, "reply_to": -1, "tag": null, "text": "umm那dz对CNN/MLP的反向传递、更新权重这些东西都了解吗", "timestamp": 1627876004, "type": "text", "url": ""}, {"cid": 2189507, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Dave", "pid": 424210, "reply_to": 2189506, "tag": null, "text": "Re Dave: 指数学层面上", "timestamp": 1627876019, "type": "text", "url": ""}, {"cid": 2189509, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2189506, "tag": null, "text": "Re Dave: 公式没推过，大致知道这个过程。", "timestamp": 1627876035, "type": "text", "url": ""}, {"cid": 2189516, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Dave", "pid": 424210, "reply_to": 2189509, "tag": null, "text": "Re 洞主: 🤔可以尝试自己用numpy手动实现一个三层的MLP（就只要全连接层就好），会对这个有更深入的理解！", "timestamp": 1627876100, "type": "text", "url": ""}, {"cid": 2189519, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2189487, "tag": null, "text": "Re Carol: 这个计算图内存是属于哪个对象呢。如果属于model，那为什么调用一个并没有以model为参数传入的loss能更新model的参数呢？", "timestamp": 1627876128, "type": "text", "url": ""}, {"cid": 2189521, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2189516, "tag": null, "text": "Re Dave: 谢谢Dave的建议。我加油😂", "timestamp": 1627876154, "type": "text", "url": ""}, {"cid": 2189526, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Carol", "pid": 424210, "reply_to": 2189516, "tag": null, "text": "Re Dave: dz的问题不是原理吧，原理学过微积分都很好理解，感觉她在问pytorch怎么实现的orz", "timestamp": 1627876196, "type": "text", "url": ""}, {"cid": 2189533, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Dave", "pid": 424210, "reply_to": 2189526, "tag": null, "text": "Re Carol: ummmm理解原理和手动搭出来之间感觉还差蛮多东西的，而且自己手动搭一遍之后对于阅读源码也有好处", "timestamp": 1627876281, "type": "text", "url": ""}, {"cid": 2189537, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2189487, "tag": null, "text": "Re Carol: 这个代码给我的感觉是，model=1，loss=targets-model，loss.backward()，然后现在告诉我print(model)是2了😂，好奇怪", "timestamp": 1627876303, "type": "text", "url": ""}, {"cid": 2189540, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2189526, "tag": null, "text": "Re Carol: 对对，我很想知道怎么实现的", "timestamp": 1627876339, "type": "text", "url": ""}, {"cid": 2189550, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Dave", "pid": 424210, "reply_to": 2189526, "tag": null, "text": "Re Carol: 万一dz之后有更多的疑问呢，比如minibatch是怎么回事、dataloader是怎么回事、数据增强又是怎么搞的这样的", "timestamp": 1627876382, "type": "text", "url": ""}, {"cid": 2189580, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Alice", "pid": 424210, "reply_to": 2189540, "tag": null, "text": "Re 洞主: 一路钻下去那得到最底层的C++源码", "timestamp": 1627876723, "type": "text", "url": ""}, {"cid": 2189664, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Eve", "pid": 424210, "reply_to": -1, "tag": null, "text": "dz这running_loss=...的那一行，不会引入新的计算图吗", "timestamp": 1627877322, "type": "text", "url": ""}, {"cid": 2189666, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Eve", "pid": 424210, "reply_to": -1, "tag": null, "text": "不要加.detach吗", "timestamp": 1627877354, "type": "text", "url": ""}, {"cid": 2189695, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Francis", "pid": 424210, "reply_to": 2189666, "tag": null, "text": "Re Eve: 对，loss这种标量也可以加.item()，变native的float", "timestamp": 1627877661, "type": "text", "url": ""}, {"cid": 2189736, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Francis", "pid": 424210, "reply_to": 2189509, "tag": null, "text": "Re 洞主: 到底更新哪个参数是在torch.optim.SGD(model.parameters())这里指定的。你可以把这行改成SGD(list(model.parameters())[0])，这样其他参数都不会变，只有第0个参数会被优化。", "timestamp": 1627878042, "type": "text", "url": ""}, {"cid": 2189748, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Francis", "pid": 424210, "reply_to": 2189509, "tag": null, "text": "loss.backward()的时候是不会优化参数的，只是根据计算图算出来每个parameter的.grad，optimizer.step()的时候才会", "timestamp": 1627878142, "type": "text", "url": ""}, {"cid": 2189795, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": -1, "tag": null, "text": "谢谢诸位。dz刚入门，看了231有些心急，上来就直接构建神经网络了。对pytorch的计算图和动态图机制不太了解，先去补补有问题再来请教🤔", "timestamp": 1627878763, "type": "text", "url": ""}, {"cid": 2189796, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2189748, "tag": null, "text": "Re Francis: 谢谢 francis，我结合博客再看看。", "timestamp": 1627878787, "type": "text", "url": ""}, {"cid": 2189827, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Grace", "pid": 424210, "reply_to": -1, "tag": null, "text": "原来文科生这么强", "timestamp": 1627879151, "type": "text", "url": ""}, {"cid": 2190212, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": -1, "tag": null, "text": "各位dy，弄明白了。 可微tensor运算出来的变量如y=torch.mul(w,x)有成员y.grad_fn，保存了y和wx的函数关系，构建了一个回溯机制。第一个问题解决了。\n第二个问题，model.parameters()返回的是一个generator类型，根据我查到的博客这个类似一个不可变的list，虽然返回的值是不可变的，但返回的是类型为tensor的参数，运用上面的回溯机制，就能改变这些参数的值。\n可能表述不严谨，但感觉就是这个意思。终于明白了，很开心。\n再次谢谢各位！", "timestamp": 1627882608, "type": "text", "url": ""}, {"cid": 2191407, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Hans", "pid": 424210, "reply_to": -1, "tag": null, "text": "pytorch 的autograd机制，看看官网的tutoria就明白了\n", "timestamp": 1627893577, "type": "text", "url": ""}, {"cid": 2191414, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2191407, "tag": null, "text": "Re Hans: 嗯嗯，谢谢hans！", "timestamp": 1627893652, "type": "text", "url": ""}, {"cid": 2193426, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Isabella", "pid": 424210, "reply_to": -1, "tag": null, "text": "可以理解为result_loss里保存了model所有参数的引用，因此对result_loss调用backward的时候，可以为所有model的参数计算到梯度。引用的构建方法就是用dz提到的那个grad_fn。pytorch神经网络里面的参数就像一个图结构，图里每个点都是参数，边是参数之间的计算公式，因此末尾的loss节点可以回溯到前面的每一个参数", "timestamp": 1627913407, "type": "text", "url": ""}, {"cid": 2193431, "deleted": false, "image_metadata": {}, "is_dz": true, "name": "洞主", "pid": 424210, "reply_to": 2193426, "tag": null, "text": "Re Isabella: 谢谢Isabella！我就这么理解吧😁", "timestamp": 1627913459, "type": "text", "url": ""}, {"cid": 2197596, "deleted": false, "image_metadata": {}, "is_dz": false, "name": "Jason", "pid": 424210, "reply_to": 2193431, "tag": null, "text": "Re 洞主: 虽然没有深究pytorch的实现，但是想来应该是所有存在于计算图里面的tensor都会保留上游tensor的引用，backward之后就可以层层回溯，计算grad。optimizer.step就会遍历所有传入optimizer的可优化tensor，根据每个tensor的grad更新。", "timestamp": 1627971637, "type": "text", "url": ""}], "post": {"deleted": false, "image_metadata": {"h": 686, "w": 1164}, "likenum": 20, "pid": 424210, "reply": 35, "tag": null, "text": "文科生想请教一个pytoch的问题。\n\n我想知道，这个result_loss.backward()为什么会改变model里面的grad呢。result_loss是outputs和targets计算返回的一个tensor，从语法来讲，和model是没有关系的呀。", "timestamp": 1627875553, "type": "image", "updated_at": 1627971638, "url": "jv/jvhu6f3je7c6rmoj5cbdazdzs5c3ww4p.png", "vote": {}}}